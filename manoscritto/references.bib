@book{SuttonBarto,
  author    = {Richard S. Sutton and Andrew G. Barto},
  title     = {Reinforcement Learning: An Introduction},
  year      = {2018},
  publisher = {MIT Press},
  address   = {Cambridge, Massachusetts (MA)},
  url       = {http://incompleteideas.net/book/the-book.html}
}

@book{BellmanDP,
  title     = {Dynamic Programming},
  author    = {Richard E. Bellman},
  publisher = {Princeton University Press},
  isbn      = {0-691-07951-X},
  year      = {1957},
  series    = {Princeton Landmarks in Mathematics and Physics},
  edition   = {},
  volume    = {}
}

@InCollection{Spaan12pomdp,
  author    = {Matthijs T. J. Spaan},
  title     = {Partially Observable Markov Decision Processes},
  editor    = {Marco Wiering and Martijn van Otterlo},
  booktitle = {Reinforcement Learning: State of the Art},
  publisher = {Springer Verlag},
  year      = {2012},
  pages     = {387--414}
}

@article{Kaelbling1998,
  author = {Leslie Pack Kaelbling and Michael L. Littman and Anthony R. Cassandra},
  title     = {Planning and acting in partially observable stochastic domains},
  journal   = {Artificial Intelligence},
  volume    = {101},
  number    = {1},
  pages     = {99-134},
  year      = {1998},
  issn      = {0004-3702},
  doi       = {https://doi.org/10.1016/S0004-3702(98)00023-X},
}
%  url       = {https://www.sciencedirect.com/science/article/pii/S000437029800023X},
%  keywords  = {Planning, Uncertainty, Partially observable Markov decision processes},
%  abstract  = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (mdps) and partially observable MDPs (pomdps). We then outline a novel algorithm for solving pomdps off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to pomdps, and of some possibilities for finding approximate solutions.}
%}

@book{WierOtter12RLStateOfTheArt,
  author    = {Marco Wiering and Martijn van Otterlo},
  title     = {Reinforcement Learning: State of the Art},
  publisher = {Springer Verlag},
  year      = {2012},
  isbn      = {ISBN: 978-3-642-27645-3},
}

@Inbook{Peters2010,
  author    = {Peters, Jan and Bagnell, J. Andrew},
  title     = {Policy Gradient Methods},
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  booktitle = {Encyclopedia of Machine Learning},
  year      = {2010},
  publisher = {Springer US},
  address   = {Boston, MA},
  pages     = {774--776},
  isbn      = {978-0-387-30164-8},
  doi       = {10.1007/978-0-387-30164-8_640},
}
%  url       = {https://doi.org/10.1007/978-0-387-30164-8_640}
%}

@Inbook{Uther2010,
  author    = {Uther, William},
  title     = {Markov Decision Processes},
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  booktitle = {Encyclopedia of Machine Learning},
  year      = {2010},
  publisher = {Springer US},
  address   = {Boston, MA},
  pages     = {642--646},
  isbn      = {978-0-387-30164-8},
  doi       = {10.1007/978-0-387-30164-8_512},
}
%  url       = {https://doi.org/10.1007/978-0-387-30164-8_512}
%}

@Inbook{Poupart2010,
  author    = {Poupart, Pascal},
  title     = {Partially Observable Markov Decision Processes},
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  booktitle = {Encyclopedia of Machine Learning},
  year      = {2010},
  publisher = {Springer US},
  address   = {Boston, MA},
  pages     = {776--776},
  isbn      = {978-0-387-30164-8},
  doi       = {10.1007/978-0-387-30164-8_642},
}
%  url       = {https://doi.org/10.1007/978-0-387-30164-8_642}
%}


@inproceedings{Aberdeen2002ScalingIP,
  author    = {D. Aberdeen and Jonathan Baxter},
  title     = {Scaling Internal-State Policy-Gradient Methods for POMDPs},
  year      = {2002},
  booktitle = {International Conference on Machine Learning}
}

@article{Peters2008,
  author    = {Jan Peters and Stefan Schaal},
  title     = {Natural Actor-Critic},
  journal   = {Neurocomputing},
  volume    = {71},
  number    = {7},
  pages     = {1180-1190},
  year      = {2008},
  issn      = {0925-2312},
  doi       = {10.1016/j.neucom.2007.11.026},
}
%  url       = {https://www.sciencedirect.com/science/article/pii/S0925231208000532},
%  note      = {Progress in Modeling, Theory, and Application of Computational Intelligenc},
%  keywords  = {Policy-gradient methods, Compatible function approximation, Natural gradients, Actor-Critic methods, Reinforcement learning, Robot learning},
%  abstract  = {In this paper, we suggest a novel reinforcement learning architecture, the Natural Actor-Critic. The actor updates are achieved using stochastic policy gradients employing Amari's natural gradient approach, while the critic obtains both the natural policy gradient and additional parameters of a value function simultaneously by linear regression. We show that actor improvements with natural policy gradients are particularly appealing as these are independent of coordinate frame of the chosen policy representation, and can be estimated more efficiently than regular policy gradients. The critic makes use of a special basis function parameterization motivated by the policy-gradient compatible function approximation. We show that several well-known reinforcement learning methods such as the original Actor-Critic and Bradtke's Linear Quadratic Q-Learning are in fact Natural Actor-Critic algorithms. Empirical evaluations illustrate the effectiveness of our techniques in comparison to previous methods, and also demonstrate their applicability for learning control on an anthropomorphic robot arm.}
%}

@inproceedings{Sutton2000,
  author    = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  title     = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {S. Solla and T. Leen and K. M\"{u}ller},
  pages     = {},
  publisher = {MIT Press},
  volume    = {12},
  year      = {2000}
}
%  url       = {https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf},
%}

@book{Montresor2014,
	title     = {Algoritmi e strutture dati},
	author    = {Bertossi, Alan and Montresor, Alberto},
	publisher = {Citt√†Studi Edizioni},
	address   = {},
	isbn      = {9788825173956},
	year      = {2014},
	series    = {},
	edition   = {Terza edizione},
	volume    = {},
	language  = {},
	url       = {},
	note      = {}
}