@book{SuttonBarto,
  author    = {Richard S. Sutton and Andrew G. Barto},
  title     = {Reinforcement Learning: An Introduction},
  year      = {2018},
  publisher = {MIT Press},
  address   = {Cambridge, Massachusetts (MA)},
  url       = {http://incompleteideas.net/book/the-book.html}
}

@book{BellmanDP,
  title     = {Dynamic Programming},
  author    = {Richard E. Bellman},
  publisher = {Princeton University Press},
  isbn      = {0-691-07951-X},
  year      = {1957},
  series    = {Princeton Landmarks in Mathematics and Physics},
  edition   = {},
  volume    = {}
}

@InCollection{Spaan12pomdp,
  author    = {Matthijs T. J. Spaan},
  title     = {Partially Observable Markov Decision Processes},
  editor    = {Marco Wiering and Martijn van Otterlo},
  booktitle = {Reinforcement Learning: State of the Art},
  publisher = {Springer Verlag},
  year      = {2012},
  pages     = {387--414}
}

@article{Kaelbling1998,
  author = {Leslie Pack Kaelbling and Michael L. Littman and Anthony R. Cassandra},
  title     = {Planning and acting in partially observable stochastic domains},
  journal   = {Artificial Intelligence},
  volume    = {101},
  number    = {1},
  pages     = {99-134},
  year      = {1998},
  issn      = {0004-3702},
  doi       = {https://doi.org/10.1016/S0004-3702(98)00023-X},
}
%  url       = {https://www.sciencedirect.com/science/article/pii/S000437029800023X},
%  keywords  = {Planning, Uncertainty, Partially observable Markov decision processes},
%  abstract  = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (mdps) and partially observable MDPs (pomdps). We then outline a novel algorithm for solving pomdps off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to pomdps, and of some possibilities for finding approximate solutions.}
%}

@book{WierOtter12RLStateOfTheArt,
  author    = {Marco Wiering and Martijn van Otterlo},
  title     = {Reinforcement Learning: State of the Art},
  publisher = {Springer Verlag},
  year      = {2012},
  isbn      = {ISBN: 978-3-642-27645-3},
}

@Inbook{Peters2010,
  author    = {Peters, Jan and Bagnell, J. Andrew},
  title     = {Policy Gradient Methods},
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  booktitle = {Encyclopedia of Machine Learning},
  year      = {2010},
  publisher = {Springer US},
  address   = {Boston, MA},
  pages     = {774--776},
  isbn      = {978-0-387-30164-8},
  doi       = {10.1007/978-0-387-30164-8_640},
}
%  url       = {https://doi.org/10.1007/978-0-387-30164-8_640}
%}

@Inbook{Uther2010,
  author    = {Uther, William},
  title     = {Markov Decision Processes},
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  booktitle = {Encyclopedia of Machine Learning},
  year      = {2010},
  publisher = {Springer US},
  address   = {Boston, MA},
  pages     = {642--646},
  isbn      = {978-0-387-30164-8},
  doi       = {10.1007/978-0-387-30164-8_512},
}
%  url       = {https://doi.org/10.1007/978-0-387-30164-8_512}
%}

@Inbook{Poupart2010,
  author    = {Poupart, Pascal},
  title     = {Partially Observable Markov Decision Processes},
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  booktitle = {Encyclopedia of Machine Learning},
  year      = {2010},
  publisher = {Springer US},
  address   = {Boston, MA},
  pages     = {776--776},
  isbn      = {978-0-387-30164-8},
  doi       = {10.1007/978-0-387-30164-8_642},
}
%  url       = {https://doi.org/10.1007/978-0-387-30164-8_642}
%}

@inproceedings{Aberdeen2002ScalingIP,
  author    = {D. Aberdeen and Jonathan Baxter},
  title     = {Scaling Internal-State Policy-Gradient Methods for POMDPs},
  year      = {2002},
  booktitle = {International Conference on Machine Learning}
}

@article{Peters2008,
  author    = {Jan Peters and Stefan Schaal},
  title     = {Natural Actor-Critic},
  journal   = {Neurocomputing},
  volume    = {71},
  number    = {7},
  pages     = {1180-1190},
  year      = {2008},
  issn      = {0925-2312},
  doi       = {10.1016/j.neucom.2007.11.026},
}
%  url       = {https://www.sciencedirect.com/science/article/pii/S0925231208000532},
%  note      = {Progress in Modeling, Theory, and Application of Computational Intelligenc},
%  keywords  = {Policy-gradient methods, Compatible function approximation, Natural gradients, Actor-Critic methods, Reinforcement learning, Robot learning},
%  abstract  = {In this paper, we suggest a novel reinforcement learning architecture, the Natural Actor-Critic. The actor updates are achieved using stochastic policy gradients employing Amari's natural gradient approach, while the critic obtains both the natural policy gradient and additional parameters of a value function simultaneously by linear regression. We show that actor improvements with natural policy gradients are particularly appealing as these are independent of coordinate frame of the chosen policy representation, and can be estimated more efficiently than regular policy gradients. The critic makes use of a special basis function parameterization motivated by the policy-gradient compatible function approximation. We show that several well-known reinforcement learning methods such as the original Actor-Critic and Bradtke's Linear Quadratic Q-Learning are in fact Natural Actor-Critic algorithms. Empirical evaluations illustrate the effectiveness of our techniques in comparison to previous methods, and also demonstrate their applicability for learning control on an anthropomorphic robot arm.}
%}

@inproceedings{Sutton2000,
  author    = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  title     = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {S. Solla and T. Leen and K. M\"{u}ller},
  pages     = {},
  publisher = {MIT Press},
  volume    = {12},
  year      = {2000}
}
%  url       = {https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf},
%}

@inproceedings{Hennes2020,
  author    = {Hennes, Daniel and Morrill, Dustin and Omidshafiei, Shayegan and Munos, R\'{e}mi and Perolat, Julien and Lanctot, Marc and Gruslys, Audrunas and Lespiau, Jean-Baptiste and Parmas, Paavo and Du\`{e}\~{n}ez-Guzm\'{a}n, Edgar and Tuyls, Karl},
  title     = {Neural Replicator Dynamics: Multiagent Learning via Hedging Policy Gradients},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  year      = {2020},
  isbn      = {9781450375184},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  address   = {Richland, SC},
  pages     = {492â€“501},
  numpages  = {10},
  location  = {Auckland, New Zealand},
  series    = {AAMAS '20},
}
%  abstract  = {Policy gradient and actor-critic algorithms form the basis of many commonly used training techniques in deep reinforcement learning. Using these algorithms in multiagent environments poses problems such as nonstationarity and instability. In this paper, we first demonstrate that standard softmax-based policy gradient can be prone to poor performance in the presence of even the most benign nonstationarity. By contrast, it is known that the replicator dynamics, a well-studied model from evolutionary game theory, eliminates dominated strategies and exhibits convergence of the time-averaged trajectories to interior Nash equilibria in zero-sum games. Thus, using the replicator dynamics as a foundation, we derive an elegant one-line change to policy gradient methods that simply bypasses the gradient step through the softmax, yielding a new algorithm titled Neural Replicator Dynamics (NeuRD). NeuRD reduces to the exponential weights/Hedge algorithm in the single-state all-actions case. Additionally, NeuRD has formal equivalence to softmax counterfactual regret minimization, which guarantees convergence in the sequential tabular case. Importantly, our algorithm provides a straightforward way of extending the replicator dynamics to the function approximation setting. Empirical results show that NeuRD quickly adapts to nonstationarities, outperforming policy gradient significantly in both tabular and function approximation settings, when evaluated on the standard imperfect information benchmarks of Kuhn Poker, Leduc Poker, and Goofspiel.},
%  keywords = {multiagent, reinforcement learning, regret minimization, games}
%}

@article{Amari1998,
  author    = {Amari, Shun-ichi},
  title     = {Natural Gradient Works Efficiently in Learning},
  journal   = {Neural Computation},
  volume    = {10},
  number    = {2},
  pages     = {251-276},
  year      = {1998},
  month     = {02},
  issn      = {0899-7667},
  doi       = {10.1162/089976698300017746},
}
%  eprint    = {https://direct.mit.edu/neco/article-pdf/10/2/251/813415/089976698300017746.pdf},
%  url       = {https://doi.org/10.1162/089976698300017746},
%  abstract  = {When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed.},
%}

@book{Montresor2014,
	title     = {Algoritmi e strutture dati},
	author    = {Bertossi, Alan and Montresor, Alberto},
	publisher = {CittÃ Studi Edizioni},
	address   = {},
	isbn      = {9788825173956},
	year      = {2014},
	series    = {},
	edition   = {Terza edizione},
	volume    = {},
	language  = {},
	url       = {},
	note      = {}
}